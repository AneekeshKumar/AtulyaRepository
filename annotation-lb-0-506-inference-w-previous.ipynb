{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aneekeshkumar/annotation-lb-0-506-inference-w-previous?scriptVersionId=155869153\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"aa06b56b","metadata":{"execution":{"iopub.execute_input":"2022-08-31T14:41:32.386121Z","iopub.status.busy":"2022-08-31T14:41:32.385678Z","iopub.status.idle":"2022-08-31T14:41:33.84764Z","shell.execute_reply":"2022-08-31T14:41:33.84594Z","shell.execute_reply.started":"2022-08-31T14:41:32.386036Z"},"papermill":{"duration":0.006652,"end_time":"2023-12-20T22:32:02.886505","exception":false,"start_time":"2023-12-20T22:32:02.879853","status":"completed"},"tags":[]},"source":[" \n","\n","##### This notebook is a fork of [this notebook](https://www.kaggle.com/code/mbmmurad/lb-0-506-inference-w-previous-comp-winner-s-model/notebook).\n","\n","\n","💗This notebook has made some annotations on the original code. If it is useful to you, please click like. Thank you!💗\n"]},{"cell_type":"code","execution_count":1,"id":"c3eb04c1","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:32:02.900305Z","iopub.status.busy":"2023-12-20T22:32:02.899938Z","iopub.status.idle":"2023-12-20T22:32:04.15797Z","shell.execute_reply":"2023-12-20T22:32:04.156777Z"},"papermill":{"duration":1.26764,"end_time":"2023-12-20T22:32:04.160452","exception":false,"start_time":"2023-12-20T22:32:02.892812","status":"completed"},"tags":[]},"outputs":[],"source":["!cp -r ../input/python-packages2 ./"]},{"cell_type":"code","execution_count":2,"id":"d3ee9e45","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:32:04.174423Z","iopub.status.busy":"2023-12-20T22:32:04.174102Z","iopub.status.idle":"2023-12-20T22:33:43.36862Z","shell.execute_reply":"2023-12-20T22:33:43.367556Z"},"papermill":{"duration":99.20442,"end_time":"2023-12-20T22:33:43.37111","exception":false,"start_time":"2023-12-20T22:32:04.16669","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["jiwer/\r\n","jiwer/jiwer-2.3.0-py3-none-any.whl\r\n","jiwer/python-Levenshtein-0.12.2.tar.gz\r\n","jiwer/setuptools-65.3.0-py3-none-any.whl\r\n","Looking in links: ./\r\n","Processing ./jiwer/jiwer-2.3.0-py3-none-any.whl\r\n","INFO: pip is looking at multiple versions of jiwer to determine which version is compatible with other requirements. This could take a while.\r\n","\u001b[31mERROR: Could not find a version that satisfies the requirement python-Levenshtein==0.12.2 (from jiwer) (from versions: none)\u001b[0m\u001b[31m\r\n","\u001b[0m\u001b[31mERROR: No matching distribution found for python-Levenshtein==0.12.2\u001b[0m\u001b[31m\r\n","\u001b[0mnormalizer/\r\n","normalizer/bnunicodenormalizer-0.0.24.tar.gz\r\n","Looking in links: ./\r\n","Processing ./normalizer/bnunicodenormalizer-0.0.24.tar.gz\r\n","  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25hBuilding wheels for collected packages: bnunicodenormalizer\r\n","  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.0.24-py3-none-any.whl size=17628 sha256=021745b3f0ca9d287b6ac39a35b5aab20a5b46ca196fe0fc459c9e71bfeeb058\r\n","  Stored in directory: /root/.cache/pip/wheels/78/d7/75/6986dc3616718f950b80e3bd79a796ef618eaef6cd800e7909\r\n","Successfully built bnunicodenormalizer\r\n","Installing collected packages: bnunicodenormalizer\r\n","Successfully installed bnunicodenormalizer-0.0.24\r\n","pyctcdecode/\r\n","pyctcdecode/hypothesis-6.54.4-py3-none-any.whl\r\n","pyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl\r\n","pyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl\r\n","pyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl\r\n","pyctcdecode/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n","pyctcdecode/attrs-22.1.0-py2.py3-none-any.whl\r\n","pyctcdecode/pygtrie-2.5.0.tar.gz\r\n","Looking in links: ./\r\n","Processing ./pyctcdecode/attrs-22.1.0-py2.py3-none-any.whl\r\n","Installing collected packages: attrs\r\n","  Attempting uninstall: attrs\r\n","    Found existing installation: attrs 23.1.0\r\n","    Uninstalling attrs-23.1.0:\r\n","      Successfully uninstalled attrs-23.1.0\r\n","Successfully installed attrs-22.1.0\r\n","Looking in links: ./\r\n","Processing ./pyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl\r\n","Installing collected packages: exceptiongroup\r\n","  Attempting uninstall: exceptiongroup\r\n","    Found existing installation: exceptiongroup 1.1.1\r\n","    Uninstalling exceptiongroup-1.1.1:\r\n","      Successfully uninstalled exceptiongroup-1.1.1\r\n","Successfully installed exceptiongroup-1.0.0rc9\r\n","Looking in links: ./\r\n","Processing ./pyctcdecode/hypothesis-6.54.4-py3-none-any.whl\r\n","Installing collected packages: hypothesis\r\n","Successfully installed hypothesis-6.54.4\r\n","Looking in links: ./\r\n","\u001b[31mERROR: numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\r\n","\u001b[0mLooking in links: ./\r\n","Processing ./pyctcdecode/pygtrie-2.5.0.tar.gz\r\n","  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25hBuilding wheels for collected packages: pygtrie\r\n","  Building wheel for pygtrie (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25h  Created wheel for pygtrie: filename=pygtrie-2.5.0-py3-none-any.whl size=20945 sha256=91cae2a34e1c8d68d20f4067b21056dc64dd2c869475ca33d2dc264745f5a9b2\r\n","  Stored in directory: /root/.cache/pip/wheels/78/28/09/b62c97a3e77102645c7ecc78c97580ad57090b1eee5438d6ac\r\n","Successfully built pygtrie\r\n","Installing collected packages: pygtrie\r\n","Successfully installed pygtrie-2.5.0\r\n","Looking in links: ./\r\n","Processing ./pyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl\r\n","sortedcontainers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n","Looking in links: ./\r\n","Processing ./pyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl\r\n","Installing collected packages: pyctcdecode\r\n","Successfully installed pyctcdecode-0.4.0\r\n","pypikenlm/\r\n","pypikenlm/pypi-kenlm-0.1.20220713.tar.gz\r\n","Looking in links: ./\r\n","Processing ./pypikenlm/pypi-kenlm-0.1.20220713.tar.gz\r\n","  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n","\u001b[?25hBuilding wheels for collected packages: pypi-kenlm\r\n","  Building wheel for pypi-kenlm (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n","\u001b[?25h  Created wheel for pypi-kenlm: filename=pypi_kenlm-0.1.20220713-cp310-cp310-linux_x86_64.whl size=333264 sha256=62a028ad7ee29e3c7ebd6806f102632a0994c40b505d28c122942fd8c66e5105\r\n","  Stored in directory: /root/.cache/pip/wheels/1e/7a/db/27645fac296d5d5ba5c461b1af834eebc0ba4643290dbc5476\r\n","Successfully built pypi-kenlm\r\n","Installing collected packages: pypi-kenlm\r\n","Successfully installed pypi-kenlm-0.1.20220713\r\n","Requirement already satisfied: pyctcdecode in /opt/conda/lib/python3.10/site-packages (0.4.0)\r\n","Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from pyctcdecode) (1.23.5)\r\n","Requirement already satisfied: pygtrie<3.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from pyctcdecode) (2.5.0)\r\n","Requirement already satisfied: hypothesis<7,>=6.14 in /opt/conda/lib/python3.10/site-packages (from pyctcdecode) (6.54.4)\r\n","Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (22.1.0)\r\n","Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\r\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.0.0rc9)\r\n"]}],"source":["!tar xvfz ./python-packages2/jiwer.tgz\n","!pip install ./jiwer/jiwer-2.3.0-py3-none-any.whl -f ./ --no-index\n","!tar xvfz ./python-packages2/normalizer.tgz\n","!pip install ./normalizer/bnunicodenormalizer-0.0.24.tar.gz -f ./ --no-index\n","!tar xvfz ./python-packages2/pyctcdecode.tgz\n","!pip install ./pyctcdecode/attrs-22.1.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n","!pip install ./pyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl -f ./ --no-index --no-deps\n","!pip install ./pyctcdecode/hypothesis-6.54.4-py3-none-any.whl -f ./ --no-index --no-deps\n","!pip install ./pyctcdecode/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl -f ./ --no-index --no-deps\n","!pip install ./pyctcdecode/pygtrie-2.5.0.tar.gz -f ./ --no-index --no-deps\n","!pip install ./pyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n","!pip install ./pyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n","\n","!tar xvfz ./python-packages2/pypikenlm.tgz\n","!pip install ./pypikenlm/pypi-kenlm-0.1.20220713.tar.gz -f ./ --no-index --no-deps\n","!pip install pyctcdecode\n"]},{"cell_type":"code","execution_count":3,"id":"c85be292","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:33:43.397651Z","iopub.status.busy":"2023-12-20T22:33:43.397326Z","iopub.status.idle":"2023-12-20T22:33:59.116254Z","shell.execute_reply":"2023-12-20T22:33:59.115491Z"},"papermill":{"duration":15.734538,"end_time":"2023-12-20T22:33:59.118465","exception":false,"start_time":"2023-12-20T22:33:43.383927","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["import os\n","import numpy as np\n","from tqdm.auto import tqdm\n","from glob import glob\n","from transformers import AutoFeatureExtractor, pipeline\n","import pandas as pd\n","import librosa\n","import IPython\n","from datasets import load_metric\n","from tqdm.auto import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import gc\n","import wave\n","from scipy.io import wavfile\n","import scipy.signal as sps\n","import pyctcdecode\n","\n","tqdm.pandas()\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n"]},{"cell_type":"code","execution_count":4,"id":"abee4afc","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:33:59.145018Z","iopub.status.busy":"2023-12-20T22:33:59.144366Z","iopub.status.idle":"2023-12-20T22:33:59.148634Z","shell.execute_reply":"2023-12-20T22:33:59.147812Z"},"papermill":{"duration":0.019487,"end_time":"2023-12-20T22:33:59.150572","exception":false,"start_time":"2023-12-20T22:33:59.131085","status":"completed"},"tags":[]},"outputs":[],"source":["# CHANGE ACCORDINGLY\n","BATCH_SIZE = 1\n","TEST_DIRECTORY = '/kaggle/input/bengaliai-speech/test_mp3s'"]},{"cell_type":"code","execution_count":5,"id":"c3e74c24","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:33:59.17686Z","iopub.status.busy":"2023-12-20T22:33:59.176612Z","iopub.status.idle":"2023-12-20T22:33:59.180529Z","shell.execute_reply":"2023-12-20T22:33:59.17972Z"},"papermill":{"duration":0.019253,"end_time":"2023-12-20T22:33:59.182442","exception":false,"start_time":"2023-12-20T22:33:59.163189","status":"completed"},"tags":[]},"outputs":[],"source":["\n","class CFG:\n","    my_model_name = '../input/yellowking-dlsprint-model/YellowKing_model'\n","    processor_name = '../input/yellowking-dlsprint-model/YellowKing_processor'"]},{"cell_type":"code","execution_count":6,"id":"110b7290","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:33:59.208419Z","iopub.status.busy":"2023-12-20T22:33:59.208129Z","iopub.status.idle":"2023-12-20T22:35:29.614801Z","shell.execute_reply":"2023-12-20T22:35:29.613881Z"},"papermill":{"duration":90.422052,"end_time":"2023-12-20T22:35:29.617114","exception":false,"start_time":"2023-12-20T22:33:59.195062","status":"completed"},"tags":[]},"outputs":[],"source":["from transformers import Wav2Vec2ProcessorWithLM\n","\n","processor = Wav2Vec2ProcessorWithLM.from_pretrained(CFG.processor_name)\n"]},{"cell_type":"code","execution_count":7,"id":"265ee072","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:29.644117Z","iopub.status.busy":"2023-12-20T22:35:29.643817Z","iopub.status.idle":"2023-12-20T22:35:47.373187Z","shell.execute_reply":"2023-12-20T22:35:47.372354Z"},"papermill":{"duration":17.745594,"end_time":"2023-12-20T22:35:47.375568","exception":false,"start_time":"2023-12-20T22:35:29.629974","status":"completed"},"tags":[]},"outputs":[],"source":["my_asrLM = pipeline(\"automatic-speech-recognition\", model=CFG.my_model_name ,feature_extractor =processor.feature_extractor, tokenizer= processor.tokenizer,decoder=processor.decoder ,device=0)\n"]},{"cell_type":"markdown","id":"ea377240","metadata":{"papermill":{"duration":0.012162,"end_time":"2023-12-20T22:35:47.400445","exception":false,"start_time":"2023-12-20T22:35:47.388283","status":"completed"},"tags":[]},"source":["**Following Sample Submission:**"]},{"cell_type":"code","execution_count":8,"id":"3cd48e73","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:47.426354Z","iopub.status.busy":"2023-12-20T22:35:47.426046Z","iopub.status.idle":"2023-12-20T22:35:47.433182Z","shell.execute_reply":"2023-12-20T22:35:47.432397Z"},"papermill":{"duration":0.022308,"end_time":"2023-12-20T22:35:47.435087","exception":false,"start_time":"2023-12-20T22:35:47.412779","status":"completed"},"tags":[]},"outputs":[],"source":["def infer(audio_path):\n","    speech, sr = librosa.load(audio_path, sr=processor.feature_extractor.sampling_rate)\n","\n","    my_LM_prediction = my_asrLM(\n","                speech\n","            )\n","\n","    return my_LM_prediction['text']\n","\"\"\"\n","In the provided code snippet, it appears that you are trying to implement a function named `infer` that takes an audio file path as input, performs automatic speech recognition (ASR) using a pre-trained language model (LM), and returns the predicted text from the speech. However, there are a few undefined variables and functions in the code that need clarification to understand the entire process. I'll explain the code step-by-step:\n","\n","1. `librosa.load(audio_path, sr=processor.feature_extractor.sampling_rate)`: This line uses the librosa library to load the audio file specified by `audio_path` and returns the audio waveform `speech` and the sample rate `sr`. The `processor.feature_extractor.sampling_rate` seems to be a variable or attribute that holds the desired sampling rate for the audio.\n","\n","2. `my_asrLM(speech)`: It seems like `my_asrLM` is a custom function that performs automatic speech recognition using a pre-trained language model. The input to this function is the `speech`, which is the audio waveform loaded in the previous step. This function might internally use a language model specifically trained for ASR to convert the speech into text.\n","\n","3. `my_LM_prediction['text']`: Assuming `my_asrLM` returns a dictionary containing various information about the ASR prediction, this line retrieves the recognized text from the ASR prediction result.\n","\n","Based on the provided code snippet, I can't determine the specific details of the `my_asrLM` function or the `processor.feature_extractor.sampling_rate` attribute since they are not defined in the snippet. If you can provide more context or the implementation of these functions/variables, I can help you further with the ASR inference process.\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":9,"id":"01f32cf4","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:47.461159Z","iopub.status.busy":"2023-12-20T22:35:47.460881Z","iopub.status.idle":"2023-12-20T22:35:47.469656Z","shell.execute_reply":"2023-12-20T22:35:47.468969Z"},"papermill":{"duration":0.024147,"end_time":"2023-12-20T22:35:47.471598","exception":false,"start_time":"2023-12-20T22:35:47.447451","status":"completed"},"tags":[]},"outputs":[],"source":["def batch_infer(audio_paths, batch_size=BATCH_SIZE):\n","    '''\n","    infers on a batch of audio\n","    args:\n","      audio_paths  : list of path to audio files <list of string>\n","    returns:\n","      bangla predicted texts <list of string>\n","    '''\n","    results = []\n","    for path in audio_paths:\n","        pred = \"\"\n","        try:\n","            pred = infer(path)\n","        except:\n","            pred = \"এ\"\n","        if len(pred)==0:\n","            pred = \"এ\"\n","        results.append(pred)\n","    \n","    return results\n","\n","\"\"\"\n","The provided code snippet defines a function named `batch_infer`, which performs inference on a batch of audio files using the `infer` function (assuming that the `infer` function is defined elsewhere in the code, and it processes a single audio file to obtain a predicted text). The function handles exceptions and returns the predicted texts for each audio file in the input list `audio_paths`.\n","\n","Let's break down the function step-by-step:\n","\n","1. `def batch_infer(audio_paths, batch_size=BATCH_SIZE):`: The function `batch_infer` takes two parameters: `audio_paths`, which is a list of file paths to audio files, and `batch_size` (defaulted to `BATCH_SIZE`, which should be defined elsewhere).\n","\n","2. `results = []`: Initializes an empty list named `results`, which will be used to store the predicted texts for each audio file.\n","\n","3. `for path in audio_paths:`: This loop iterates through each audio file path in the input list `audio_paths`.\n","\n","4. `pred = \"\"`: Initializes an empty string variable `pred`, which will be used to store the predicted text for the current audio file.\n","\n","5. `try:`: This block tries to execute the `infer` function with the current audio file path `path`.\n","\n","6. `pred = infer(path)`: Calls the `infer` function with the current audio file path `path` to get the predicted text for the audio file.\n","\n","7. `except:`: If an exception occurs during the execution of the `infer` function (e.g., an error or exception in the `infer` function), this block will be executed.\n","\n","8. `pred = \"এ\"`: In case of an exception, the variable `pred` is set to the Bengali character \"এ\".\n","\n","9. `if len(pred) == 0:`: This checks if the length of the predicted text `pred` is zero, which means no text was predicted for the audio file.\n","\n","10. `pred = \"এ\"`: If no text was predicted (i.e., the length is zero), the variable `pred` is set to the Bengali character \"এ\".\n","\n","11. `results.append(pred)`: The predicted text for the current audio file is added to the `results` list.\n","\n","12. `return results`: After processing all audio files in the `audio_paths` list, the function returns the `results` list, which contains the predicted texts for each audio file.\n","\n","It is important to note that the `infer` function is not defined within the provided code snippet. The `infer` function is assumed to be implemented elsewhere in the code and is responsible for processing a single audio file and returning the predicted text. The `batch_infer` function, on the other hand, handles a batch of audio files by calling the `infer` function for each audio file and collecting the results in the `results` list.\n","\"\"\";"]},{"cell_type":"code","execution_count":10,"id":"62991a90","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:47.497618Z","iopub.status.busy":"2023-12-20T22:35:47.497349Z","iopub.status.idle":"2023-12-20T22:35:47.507678Z","shell.execute_reply":"2023-12-20T22:35:47.506923Z"},"papermill":{"duration":0.02563,"end_time":"2023-12-20T22:35:47.509518","exception":false,"start_time":"2023-12-20T22:35:47.483888","status":"completed"},"tags":[]},"outputs":[],"source":["from bnunicodenormalizer import Normalizer \n","\n","\n","bnorm = Normalizer()\n","def normalize(sen):\n","    _words = [bnorm(word)['normalized']  for word in sen.split()]\n","    return \" \".join([word for word in _words if word is not None])\n","\n","def dari(sentence):\n","    try:\n","        if sentence[-1]!=\"।\":\n","            sentence+=\"।\"\n","    except:\n","        print(sentence)\n","    return sentence\n","\n","\"\"\"\n","The provided code snippet defines two functions, `normalize` and `dari`, that appear to be related to processing Bengali text. Let's break down each function:\n","\n","1. `normalize(sen)`: This function takes a Bengali sentence as input (`sen`) and returns the normalized version of the sentence. The normalization process seems to involve using the `bnunicodenormalizer` library to normalize individual words within the sentence. The function iterates through each word in the sentence, normalizes it using the `bnunicodenormalizer` library, and then joins the normalized words back into a normalized sentence. The normalized sentence will have normalized characters (e.g., combining characters) for proper rendering.\n","\n","   However, there's a small issue in the function. In the list comprehension used to normalize each word (`_words`), the normalization is attempted for every word, even if it contains characters that are not Bengali. The `bnunicodenormalizer` library is designed to work with Bengali text, so using it on non-Bengali characters may lead to unintended behavior or errors. If the input sentence contains non-Bengali characters, it's better to handle those cases explicitly.\n","\n","2. `dari(sentence)`: This function takes a Bengali sentence as input (`sentence`). It checks if the sentence ends with a Bengali full stop (U+09। - DARI). If the sentence does not end with the full stop, it appends one at the end. The purpose of this function seems to ensure that the Bengali sentence ends with the appropriate punctuation.\n","\n","   The function includes a `try-except` block, which is not necessary for this specific case. The code inside the `try` block simply checks the last character of the sentence. If the last character is not a full stop (DARI), it appends one. If the last character is already a full stop, no error will occur. Therefore, the `try-except` block is redundant, and the code can be simplified to just the `if` statement.\n","\n","It is important to note that the code relies on an external library `bnunicodenormalizer`, which is used for Bengali Unicode normalization. Ensure that you have installed this library and imported it correctly for the code to work as intended. Additionally, the code may not handle all edge cases and may need further refinement depending on the specific use case.\n","\"\"\";"]},{"cell_type":"code","execution_count":11,"id":"0b84a91b","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:47.535126Z","iopub.status.busy":"2023-12-20T22:35:47.534864Z","iopub.status.idle":"2023-12-20T22:35:47.540682Z","shell.execute_reply":"2023-12-20T22:35:47.53997Z"},"papermill":{"duration":0.020697,"end_time":"2023-12-20T22:35:47.542545","exception":false,"start_time":"2023-12-20T22:35:47.521848","status":"completed"},"tags":[]},"outputs":[],"source":["def post_process_keys(str):\n","    return str.replace(\"../input/test-wav-files-dl-sprint/test_files_wav/\",\"\").replace(\".wav\",\".mp3\")\n","\n","\"\"\"\n","The function `post_process_keys(str)` appears to be a post-processing function designed to modify and clean up file paths or keys (strings) related to audio files.\n","\n","Let's break down the function:\n","\n","1. `str.replace(\"../input/test-wav-files-dl-sprint/test_files_wav/\", \"\")`: This line of code replaces the substring `../input/test-wav-files-dl-sprint/test_files_wav/` with an empty string in the input `str`. This is essentially removing the specified prefix from the string.\n","\n","2. `.replace(\".wav\", \".mp3\")`: After removing the prefix in the previous step, this line replaces the substring `.wav` with `.mp3` in the remaining string. This is essentially changing the file extension of the audio file from WAV to MP3.\n","\n","The purpose of this function seems to be converting file paths or keys of WAV audio files to corresponding MP3 file paths or keys, possibly for further processing or handling of the audio data.\n","\n","It is important to note that modifying file paths or keys using string replacement can be error-prone, especially if the paths are not in the exact format expected by the function. Ensure that the input `str` matches the expected format, or consider adding error handling to handle unexpected inputs more gracefully. Additionally, if this function is used in a larger codebase, it's a good practice to choose a more descriptive name for the function than `post_process_keys` to reflect its specific purpose.\n","\"\"\";"]},{"cell_type":"code","execution_count":12,"id":"ed06cafe","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:47.568805Z","iopub.status.busy":"2023-12-20T22:35:47.568565Z","iopub.status.idle":"2023-12-20T22:35:47.579932Z","shell.execute_reply":"2023-12-20T22:35:47.579051Z"},"papermill":{"duration":0.026954,"end_time":"2023-12-20T22:35:47.581756","exception":false,"start_time":"2023-12-20T22:35:47.554802","status":"completed"},"tags":[]},"outputs":[],"source":["def directory_infer(audio_dir):\n","    '''\n","    infers on a directory that contains audio files\n","    args:\n","      audio_dir  : directory that contains some audio files <string>\n","    returns:\n","      a dataframe that contains 2 columns:\n","        * path <string>\n","        * sentence <string>\n","    '''\n","    # list all audio files\n","\n","    audio_paths=[audio_path for audio_path in tqdm(glob(os.path.join(audio_dir,\"*.*\")))]\n","    files = os.listdir(\"/kaggle/input/bengaliai-speech/test_mp3s\")\n","    paths = []\n","    for i in files:\n","        paths.append(i.split(\".\")[0])\n","    sentences=[]\n","    for idx in tqdm(range(0,len(audio_paths),BATCH_SIZE)):\n","        batch_paths=audio_paths[idx:idx+BATCH_SIZE]\n","        sentences+=batch_infer(batch_paths)\n","        \n","    df= pd.DataFrame({\"id\":paths,\"sentence\":sentences})\n","    df.sentence= df.sentence.apply(lambda x:normalize(x))\n","    #df.sentence= df.sentence.apply(lambda x:dari(x))\n","    df['id'] = df['id'].apply(lambda x: post_process_keys(x))\n","    \n","    return df \n","\"\"\"\n","The function `directory_infer(audio_dir)` performs inference on a directory that contains audio files. It processes the audio files in batches using the `batch_infer` function and returns the results as a DataFrame with two columns: \"id\" and \"sentence\".\n","\n","Let's break down the function step-by-step:\n","\n","1. `audio_paths = [audio_path for audio_path in tqdm(glob(os.path.join(audio_dir, \"*.*\")))]`: This line lists all the audio files in the specified `audio_dir` directory using the `glob` function. It filters all files with any extension (`*.*`) and stores their paths in the `audio_paths` list.\n","\n","2. `files = os.listdir(\"/kaggle/input/bengaliai-speech/test_mp3s\")`: This line seems to list all files in the directory \"/kaggle/input/bengaliai-speech/test_mp3s\" (hardcoded path). However, this line appears to be redundant and not directly related to the `audio_dir` parameter.\n","\n","3. `paths = []`: Initializes an empty list named `paths` to store the extracted \"id\" values from the filenames.\n","\n","4. `for i in files:`: This loop iterates through each filename in the `files` list (from step 2).\n","\n","5. `paths.append(i.split(\".\")[0])`: It splits each filename by the dot (.) and takes the first part to get the \"id\" value. The \"id\" values are then appended to the `paths` list.\n","\n","6. `sentences = []`: Initializes an empty list named `sentences` to store the predicted sentences from the ASR (Automatic Speech Recognition) model.\n","\n","7. `for idx in tqdm(range(0, len(audio_paths), BATCH_SIZE)):`: This loop iterates through the `audio_paths` list in batches of size `BATCH_SIZE` (assuming `BATCH_SIZE` is defined elsewhere).\n","\n","8. `batch_paths = audio_paths[idx:idx + BATCH_SIZE]`: Extracts a batch of audio file paths from `audio_paths`.\n","\n","9. `sentences += batch_infer(batch_paths)`: Calls the `batch_infer` function with the current batch of audio file paths and adds the predicted sentences to the `sentences` list.\n","\n","10. `df = pd.DataFrame({\"id\": paths, \"sentence\": sentences})`: Creates a DataFrame (`df`) using the \"id\" values from step 5 and the predicted sentences obtained from ASR in step 9.\n","\n","11. `df.sentence = df.sentence.apply(lambda x: normalize(x))`: Applies the `normalize` function to each sentence in the \"sentence\" column of the DataFrame. This function normalizes Bengali sentences, as explained in a previous response.\n","\n","12. `df['id'] = df['id'].apply(lambda x: post_process_keys(x))`: Applies the `post_process_keys` function to each \"id\" value in the DataFrame. This function modifies the \"id\" values, as explained in a previous response.\n","\n","13. `return df`: The function returns the DataFrame `df`, which contains the \"id\" and \"sentence\" columns with the processed data.\n","\n","It is important to note that the provided code references `BATCH_SIZE`, which is not defined in the given snippet. For this code to work correctly, `BATCH_SIZE` should be defined earlier in the code or imported from an external module. Additionally, some parts of the code (e.g., the lines related to `files`) appear to be specific to a Kaggle environment and may need modification if used in a different context.\n","\"\"\";"]},{"cell_type":"code","execution_count":13,"id":"51e1f26c","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:35:47.607872Z","iopub.status.busy":"2023-12-20T22:35:47.607629Z","iopub.status.idle":"2023-12-20T22:36:00.548269Z","shell.execute_reply":"2023-12-20T22:36:00.547358Z"},"papermill":{"duration":12.956007,"end_time":"2023-12-20T22:36:00.550212","exception":false,"start_time":"2023-12-20T22:35:47.594205","status":"completed"},"tags":[]},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e2d65c01c884e89b478a667db38b2b3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"889b5464d0584156a814fc76ec45d687","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>a9395e01ad21</td>\n","      <td>কী কারণে তুমি এতাবৎ কাল পর্যন্ত এ দারুন দৈব দু...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0f3dac00655e</td>\n","      <td>একটু বয়স হলে একটি বিদেশি।</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bf36ea8b718d</td>\n","      <td>এ কারণে সরকার নির্ধারিত হারে পরিবহন জনিত ক্ষতি...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                                           sentence\n","0  a9395e01ad21  কী কারণে তুমি এতাবৎ কাল পর্যন্ত এ দারুন দৈব দু...\n","1  0f3dac00655e                          একটু বয়স হলে একটি বিদেশি।\n","2  bf36ea8b718d  এ কারণে সরকার নির্ধারিত হারে পরিবহন জনিত ক্ষতি..."]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["submission = directory_infer(TEST_DIRECTORY)\n","submission.head()"]},{"cell_type":"code","execution_count":14,"id":"d20da2bd","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:36:00.578212Z","iopub.status.busy":"2023-12-20T22:36:00.577592Z","iopub.status.idle":"2023-12-20T22:36:00.582504Z","shell.execute_reply":"2023-12-20T22:36:00.581618Z"},"papermill":{"duration":0.020992,"end_time":"2023-12-20T22:36:00.584467","exception":false,"start_time":"2023-12-20T22:36:00.563475","status":"completed"},"tags":[]},"outputs":[],"source":["def check(sentence):\n","    if len(sentence)==0:\n","        return '।'\n","    return sentence"]},{"cell_type":"code","execution_count":15,"id":"16ea5c45","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:36:00.611178Z","iopub.status.busy":"2023-12-20T22:36:00.610898Z","iopub.status.idle":"2023-12-20T22:36:00.615505Z","shell.execute_reply":"2023-12-20T22:36:00.614666Z"},"papermill":{"duration":0.020074,"end_time":"2023-12-20T22:36:00.61734","exception":false,"start_time":"2023-12-20T22:36:00.597266","status":"completed"},"tags":[]},"outputs":[],"source":["submission.sentence = submission.sentence.apply(lambda x:check(x))"]},{"cell_type":"code","execution_count":16,"id":"d4725447","metadata":{"execution":{"iopub.execute_input":"2023-12-20T22:36:00.644016Z","iopub.status.busy":"2023-12-20T22:36:00.643746Z","iopub.status.idle":"2023-12-20T22:36:00.650336Z","shell.execute_reply":"2023-12-20T22:36:00.649706Z"},"papermill":{"duration":0.02202,"end_time":"2023-12-20T22:36:00.652235","exception":false,"start_time":"2023-12-20T22:36:00.630215","status":"completed"},"tags":[]},"outputs":[],"source":["submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","id":"2a984159","metadata":{"execution":{"iopub.execute_input":"2023-07-31T14:34:25.679144Z","iopub.status.busy":"2023-07-31T14:34:25.678782Z","iopub.status.idle":"2023-07-31T14:34:25.694089Z","shell.execute_reply":"2023-07-31T14:34:25.69288Z","shell.execute_reply.started":"2023-07-31T14:34:25.679113Z"},"papermill":{"duration":0.012581,"end_time":"2023-12-20T22:36:00.677722","exception":false,"start_time":"2023-12-20T22:36:00.665141","status":"completed"},"tags":[]},"source":["##### This notebook is a fork of [this notebook](https://www.kaggle.com/code/mbmmurad/lb-0-506-inference-w-previous-comp-winner-s-model/notebook).\n","\n","💗This notebook has made some annotations on the original code. If it is useful to you, please click like. Thank you!💗"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":6229904,"sourceId":52324,"sourceType":"competition"},{"datasetId":2308987,"sourceId":3886074,"sourceType":"datasetVersion"},{"datasetId":2311133,"sourceId":3889637,"sourceType":"datasetVersion"},{"datasetId":2446557,"sourceId":4142276,"sourceType":"datasetVersion"},{"datasetId":2447262,"sourceId":4143520,"sourceType":"datasetVersion"}],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":250.792731,"end_time":"2023-12-20T22:36:03.811651","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-20T22:31:53.01892","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0f1466d2ad6248c0ac21cda8a9af0a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7b95fe59e6c4da4b00006bac599f7e5","max":3.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_48c885f6c4314ef5823fb1be7c103ce4","value":3.0}},"19565c14b536427584298e8ef98d3b25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dcdcd78819f48dfa38db860218bde2f","placeholder":"​","style":"IPY_MODEL_b53af57f57f64252aa1ade7d0792c355","value":" 3/3 [00:00&lt;00:00, 268.43it/s]"}},"2dcdcd78819f48dfa38db860218bde2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e2d65c01c884e89b478a667db38b2b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7d84880465c40128012bc55a9ac7dbb","IPY_MODEL_0f1466d2ad6248c0ac21cda8a9af0a05","IPY_MODEL_19565c14b536427584298e8ef98d3b25"],"layout":"IPY_MODEL_4d6ed796328e4ee5909de92d55cc7321"}},"3cc2c4a8a51c44a9ab7627ed8dae1f01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48c885f6c4314ef5823fb1be7c103ce4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d6ed796328e4ee5909de92d55cc7321":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da2e5ca38db40f4a560663b81cc72af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"610d2212fa9d417ebac502e3c15b1a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6afd45a27c204a05805e5aeb0ba52be0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da2e5ca38db40f4a560663b81cc72af","max":3.0,"min":0.0,"orientation":"horizontal","style":"IPY_MODEL_f4a2a440c9204162a00e4eacac39def7","value":3.0}},"6b679243036846d281f3a4d09bf4b39d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b85a0832491e4be8be306030e4d51d89","placeholder":"​","style":"IPY_MODEL_ca5ccb14733143f68e05de8bc85ef27c","value":" 3/3 [00:12&lt;00:00,  3.36s/it]"}},"889b5464d0584156a814fc76ec45d687":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7d259f2eccf47cdbfc0bebfd99c743a","IPY_MODEL_6afd45a27c204a05805e5aeb0ba52be0","IPY_MODEL_6b679243036846d281f3a4d09bf4b39d"],"layout":"IPY_MODEL_f81a9a8145a44189bdcf957aae60201b"}},"a6a310826c3a4ce9b319b5fad6caa8f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7d84880465c40128012bc55a9ac7dbb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcf2d9b7202c41c19d9af500813ac9e3","placeholder":"​","style":"IPY_MODEL_610d2212fa9d417ebac502e3c15b1a36","value":"100%"}},"b53af57f57f64252aa1ade7d0792c355":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b85a0832491e4be8be306030e4d51d89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcf2d9b7202c41c19d9af500813ac9e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca5ccb14733143f68e05de8bc85ef27c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7b95fe59e6c4da4b00006bac599f7e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a2a440c9204162a00e4eacac39def7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7d259f2eccf47cdbfc0bebfd99c743a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cc2c4a8a51c44a9ab7627ed8dae1f01","placeholder":"​","style":"IPY_MODEL_a6a310826c3a4ce9b319b5fad6caa8f6","value":"100%"}},"f81a9a8145a44189bdcf957aae60201b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}